import requests
import time
import re
import logging
import hashlib
import sqlite3
import sys
import os
import json
from datetime import datetime
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ========== –ù–ê–°–¢–†–û–ô–ö–ê –ü–£–¢–ï–ô –î–õ–Ø –ò–ú–ü–û–†–¢–ê ==========
# –ü–æ–ª—É—á–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ —Ç–µ–∫—É—â–µ–º—É —Ñ–∞–π–ª—É (parser.py)
current_file = os.path.abspath(__file__)  # C:\Users\...\scripts\parser.py
scripts_dir = os.path.dirname(current_file)  # C:\Users\...\scripts
project_root = os.path.dirname(scripts_dir)  # C:\Users\...\project

# –ü—É—Ç—å –∫ neural_network.py
neural_path = os.path.join(project_root, 'neural_network', 'neural_network.py')

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª
if not os.path.exists(neural_path):
    print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {neural_path}")
    print("üîç –ü—Ä–æ–≤–µ—Ä—è—é –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞...")
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
    neural_dir = os.path.join(project_root, 'neural_network')
    if os.path.exists(neural_dir):
        files = os.listdir(neural_dir)
        print(f"–§–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ neural_network: {files}")

        # –ò—â–µ–º neural_network.py (–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è)
        correct_files = [f for f in files if f.lower() == 'neural_network.py']

        if correct_files:
            neural_path = os.path.join(neural_dir, correct_files[0])
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {neural_path}")
        else:
            print(f"‚ùå neural_network.py –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {neural_dir}")
            # –°–æ–∑–¥–∞–µ–º –ø—É—Ç—å –±–µ–∑ –æ–ø–µ—á–∞—Ç–∫–∏
            neural_path = os.path.join(neural_dir, 'neural_network.py')

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º neural_network —á–µ—Ä–µ–∑ importlib
try:
    import importlib.util

    spec = importlib.util.spec_from_file_location("gigachat_analysis", neural_path)
    gigachat_analysis_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(gigachat_analysis_module)
    gigachat_analysis = gigachat_analysis_module
    print(f"‚úÖ Neural network –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –∏–∑: {neural_path}")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ neural_network: {e}")
    print("üö® –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É –¥–ª—è neural_network")


    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å
    class NeuralNetworkStub:
        @staticmethod
        def start_analysis(file_path, auth_key):
            print(f"‚ö†Ô∏è –ó–∞–≥–ª—É—à–∫–∞: start_analysis({file_path}, {auth_key})")
            return {"status": "stub", "message": "Neural network not available"}

    gigachat_analysis = NeuralNetworkStub()


from dotenv import load_dotenv
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
NEWS_KEYWORDS = [
    '–Ω–æ–≤–æ—Å—Ç–∏', '–Ω–æ–≤–æ—Å—Ç—å', '–∞–≤–∞—Ä–∏—è', '–¥—Ç–ø', '–ø–æ–∂–∞—Ä', '—á–ø', '–ø—Ä–æ–∏—Å—à–µ—Å—Ç–≤–∏–µ',
    '–∑–∞–¥–µ—Ä–∂–∞–Ω–∏–µ', '–∞—Ä–µ—Å—Ç', '—Å—É–¥', '–ø–æ–ª–∏—Ü–∏—è', '–º—á—Å', '–≥–∏–±–¥–¥', '–æ—Ç—Å—Ç–∞–≤–∫–∞',
    '–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ', '–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞', '—à—Ç—Ä–∞—Ñ', '–∑–∞–ø—Ä–µ—Ç', '–∫—Ä–µ–¥–∏—Ç', '–º–∏–Ω–∏—Å—Ç—Ä',
    '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–¥–µ–≤–µ–ª–æ–ø–µ—Ä', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç', '–º–µ—Ç—Ä–æ', '–∞–≤—Ç–æ–±—É—Å',
    '—Ç—Ä–∞–º–≤–∞–π', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', '—Ä–∞–∑–≤–∏—Ç–∏–µ', '—Ñ–∏–Ω–∞–Ω—Å—ã', '–¥–æ–ª–ª–∞—Ä', '—Ä—É–±–ª—å',
    '—ç–∫–æ–Ω–æ–º–∏–∫–∞', '–±–∏–∑–Ω–µ—Å', '—Ä–µ—Å—Ç–æ—Ä–∞–Ω', '—Ä–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è', '–º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä',
    '–∑–∞—Å—Ç—Ä–æ–π—â–∏–∫', '—É–ø—Ä–∞–≤–ª—è—é—â–∏–π', '–≥–∏–ø–µ—Ä–∫–∞—Ä', '–Ω–æ—É—Ç–±—É–∫', '–∫–æ–º–ø—å—é—Ç–µ—Ä',
    '–ø–æ–¥–æ—Ä–æ–∂–∞–Ω–∏–µ', '–∫–≤–∞—Ä—Ç–∞–ª', '—Ä–∞–π–æ–Ω', '–¥–∏–∞—Å–ø–æ—Ä–∞', '–ø–∞–º—è—Ç–Ω–∏–∫',
    '—Å–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è', '–∫–æ–Ω–∫—É—Ä—Å', '–æ—Ç–ø—É—Å–∫', '–∫–∞–Ω–∏–∫—É–ª—ã', '–∏–Ω—Ç–µ—Ä–≤—å—é',
    '–æ–±—â–µ—Å—Ç–≤–æ', '–±–æ–ª—å–Ω–∏—Ü–∞', '–≥–ª–∞–≤–≤—Ä–∞—á', '–∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ', '–º—ç—Ä',
    '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–∑–∞–∫–æ–Ω', '–ø—Ä–æ–µ–∫—Ç', '–∞–Ω–∞–ª–∏—Ç–∏–∫–∞',
    '—Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '–æ–±–∑–æ—Ä', '–ø—Ä–æ–≥–Ω–æ–∑', '—Ç–µ–Ω–¥–µ–Ω—Ü–∏—è', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞',
    '–¥–∞–Ω–Ω—ã–µ', '–æ—Ç—á–µ—Ç', '–∑–∞—è–≤–ª–µ–Ω–∏–µ', '–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π', '—ç–∫—Å–ø–µ—Ä—Ç',
    '–ø—Ä–æ–≥—Ä–∞–º–º–∞', '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ', '—Ñ–æ—Ä—É–º', '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è', '—Å–æ–±—ã—Ç–∏–µ'
]

# –°–ª–æ–≤–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è (–Ω–µ –Ω–æ–≤–æ—Å—Ç–∏)
EXCLUDE_KEYWORDS = [
    '—Å–æ–≥–ª–∞—Å–∏–µ', '—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ', '—Å–æ–≥–ª–∞—Å–µ–Ω', '–ø—Ä–∏–Ω–∏–º–∞—é', '—É—Å–ª–æ–≤–∏—è', '—Ä–∞—Å—Å—ã–ª–∫—É', '–ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è',
    'cookies', '–∫—É–∫–∏', '–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö', '–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ', '–ø–æ–ª–∏—Ç–∏–∫–∞',
    '–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å', '—Ä–µ–∫–ª–∞–º–∞', '–º–∞—Ä–∫–µ—Ç–∏–Ω–≥', '–ø–æ–¥–ø–∏—Å–∫–∞', '—Ç–µ–ª–µ—Ñ–æ–Ω'
                                                              '—Ä–∞—Å—Å—ã–ª–∫–∞', '—Ä–∞—Å—Å—ã–ª–∫–∏', '–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ', '–≤–µ—Ä—Å–∏—è', 'beta',
    '—Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è', '—Ä–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä', '—É—á—Ä–µ–¥–∏—Ç–µ–ª—å',
    '—Ä–µ–¥–∞–∫—Ç–æ—Ä', '—Ä–µ–¥–∞–∫—Ü–∏—è', '—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –ø–æ—á—Ç–∞', 'appstore',
    'rustore', '–º–∏–∞', '—Ä–æ—Å—Å–∏—è —Å–µ–≥–æ–¥–Ω—è', '–ø—Ä–∞–≤–∏–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è',
    '–ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è', '–º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π',
    '—Ñ—Å77', '–º–∏–∞', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ –∞–≥–µ–Ω—Ç—Å—Ç–≤–æ', '—Å–µ—Ç–µ–≤–æ–µ –∏–∑–¥–∞–Ω–∏–µ',
    '–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ', '–≤–µ—Ä—Å–∏—è 2023', '–≤–µ—Ä—Å–∏—è 2024', '–≤–µ—Ä—Å–∏—è 2025'
]

# –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
TECHNICAL_PATTERNS = [
    r'¬©\s*\d{4}\s*.+',  # –ö–æ–ø–∏—Ä–∞–π—Ç—ã
    r'–í–µ—Ä—Å–∏—è\s+.+',  # –í–µ—Ä—Å–∏–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π
    r'–°–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ –æ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏.+',  # –°–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–∞
    r'–°–µ—Ç–µ–≤–æ–µ –∏–∑–¥–∞–Ω–∏–µ.+–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ.+',  # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –°–ú–ò
    r'–£—á—Ä–µ–¥–∏—Ç–µ–ª—å:.+',  # –£—á—Ä–µ–¥–∏—Ç–µ–ª–∏
    r'–ì–ª–∞–≤–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä:.+',  # –†–µ–¥–∞–∫—Ç–æ—Ä—ã
    r'–ê–¥—Ä–µ—Å —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –ø–æ—á—Ç—ã:.+',  # Email –∞–¥—Ä–µ—Å–∞
    r'–¢–µ–ª–µ—Ñ–æ–Ω:.+',  # –¢–µ–ª–µ—Ñ–æ–Ω—ã
    r'–§–°\d{2}-\d{5}',  # –ù–æ–º–µ—Ä —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–∞
    r'–†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä',  # –£–ø–æ–º–∏–Ω–∞–Ω–∏–µ –†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä–∞
    r'–ú–ò–ê ¬´–†–æ—Å—Å–∏—è —Å–µ–≥–æ–¥–Ω—è¬ª',  # –ù–∞–∑–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç—Å—Ç–≤–∞
    r'–≤ AppStore',  # –ú–∞–≥–∞–∑–∏–Ω—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π
    r'–≤ RuStore',  # –ú–∞–≥–∞–∑–∏–Ω—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π
    r'–ü—Ä–∞–≤–∏–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤',  # –ü—Ä–∞–≤–∏–ª–∞
    r'–ü—Ä–∞–≤–∏–ª–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.+',  # –ü—Ä–∞–≤–∏–ª–∞
    r'\d{1,2}\s+[–∞-—è]+\s+\d{4}\s+–≥–æ–¥–∞',  # –î–∞—Ç—ã —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
]


# ========== –§–£–ù–ö–¶–ò–ò –î–õ–Ø –†–ê–ë–û–¢–´ –° –ë–ê–ó–û–ô –î–ê–ù–ù–´–• ==========
# —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ, –µ—Å–ª–∏ –Ω–µ —Å–æ–∑–¥–∞–Ω–∞ –±–¥
# def init_database():
#     """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö SQLite"""
#     conn = sqlite3.connect('news_sources.db')
#     cursor = conn.cursor()
#
#     # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
#     cursor.execute('''
#     CREATE TABLE IF NOT EXISTS sources (
#         id INTEGER PRIMARY KEY AUTOINCREMENT,
#         name TEXT NOT NULL,
#         url TEXT NOT NULL,
#         type TEXT NOT NULL,
#         level TEXT,
#         theme TEXT,
#         is_active INTEGER DEFAULT 1,
#         created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
#         updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
#     )
#     ''')
#
#     # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞
#     cursor.execute('''
#     CREATE TABLE IF NOT EXISTS parsing_results (
#         id INTEGER PRIMARY KEY AUTOINCREMENT,
#         source_id INTEGER,
#         text TEXT,
#         parsing_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
#         FOREIGN KEY (source_id) REFERENCES sources (id)
#     )
#     ''')
#
#     conn.commit()
#     conn.close()
#     logger.info("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
#
#
# def import_from_excel_to_db(file_path='ekb_sources.xlsx'):
#     """–ò–º–ø–æ—Ä—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ Excel —Ñ–∞–π–ª–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
#     try:
#         # –ß–∏—Ç–∞–µ–º Excel —Ñ–∞–π–ª
#         df = pd.read_excel(file_path)
#         logger.info(f"–ü—Ä–æ—á–∏—Ç–∞–Ω–æ {len(df)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ Excel —Ñ–∞–π–ª–∞")
#
#         # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
#         conn = sqlite3.connect('news_sources.db')
#         cursor = conn.cursor()
#
#         # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É DataFrame
#         required_columns = ['–ù–∞–∑–≤–∞–Ω–∏–µ', '–°—Å—ã–ª–∫–∞', '–¢–∏–ø']
#         for col in required_columns:
#             if col not in df.columns:
#                 logger.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞: {col}")
#                 return False
#
#         # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ Excel
#         if '–£—Ä–æ–≤–µ–Ω—å' not in df.columns:
#             df['–£—Ä–æ–≤–µ–Ω—å'] = ''
#         if '–¢–µ–º–∞—Ç–∏–∫–∞' not in df.columns:
#             df['–¢–µ–º–∞—Ç–∏–∫–∞'] = ''
#
#         # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
#         cursor.execute("DELETE FROM sources")
#
#         # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
#         imported_count = 0
#         for _, row in df.iterrows():
#             cursor.execute('''
#             INSERT INTO sources (name, url, type, level, theme)
#             VALUES (?, ?, ?, ?, ?)
#             ''', (
#                 row['–ù–∞–∑–≤–∞–Ω–∏–µ'],
#                 row['–°—Å—ã–ª–∫–∞'],
#                 row['–¢–∏–ø'],
#                 row['–£—Ä–æ–≤–µ–Ω—å'],
#                 row['–¢–µ–º–∞—Ç–∏–∫–∞']
#             ))
#             imported_count += 1
#
#         conn.commit()
#         conn.close()
#
#         logger.info(f"–ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {imported_count} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
#         return True
#
#     except Exception as e:
#         logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –∏–∑ Excel: {e}")
#         return False

def get_sources_from_db():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    try:
        conn = sqlite3.connect('../data/news_sources.db')
        cursor = conn.cursor()

        cursor.execute('''
        SELECT id, name, url, type, level, theme 
        FROM sources 
        WHERE is_active = 1
        ORDER BY id
        ''')

        rows = cursor.fetchall()
        conn.close()

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º
        sources = []
        for row in rows:
            sources.append({
                'id': row[0],
                '–ù–∞–∑–≤–∞–Ω–∏–µ': row[1],
                '–°—Å—ã–ª–∫–∞': row[2],
                '–¢–∏–ø': row[3],
                '–£—Ä–æ–≤–µ–Ω—å': row[4],
                '–¢–µ–º–∞—Ç–∏–∫–∞': row[5]
            })

        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
        return sources

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ –ë–î: {e}")
        return []


def save_parsing_result_to_db(source_id, text):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
    try:
        if not text or not text.strip():
            return False

        conn = sqlite3.connect('../data/news_sources.db')
        cursor = conn.cursor()

        cursor.execute('''
        INSERT INTO parsing_results (source_id, text, parsing_time)
        VALUES (?, ?, ?)
        ''', (source_id, text.strip(), datetime.now().strftime('%Y-%m-%d %H:%M:%S')))

        # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        cursor.execute('''
        UPDATE sources 
        SET updated_at = ?
        WHERE id = ?
        ''', (datetime.now().strftime('%Y-%m-%d %H:%M:%S'), source_id))

        conn.commit()
        conn.close()
        return True

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –ë–î: {e}")
        return False


# ========== –û–°–¢–ê–í–®–ò–ï–°–Ø –§–£–ù–ö–¶–ò–ò –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô ==========

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
def create_hash(text):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ö—ç—à–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Selenium WebDriver
def init_webdriver():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è headless Chrome –¥—Ä–∞–π–≤–µ—Ä–∞"""
    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # –†–µ–∂–∏–º –±–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")

        # –î–û–ë–ê–í–¨ –≠–¢–ò –ê–†–ì–£–ú–ï–ù–¢–´ —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:
        chrome_options.add_argument("--disable-software-rasterizer")
        chrome_options.add_argument("--disable-webgl")
        chrome_options.add_argument("--disable-dev-tools")
        chrome_options.add_argument("--log-level=3")  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–æ–≤
        chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])

        chrome_options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

        # –ü–£–¢–¨ –ö CHROMEDRIVER
        scripts_dir = os.path.dirname(os.path.abspath(__file__))
        driver_path = os.path.join(scripts_dir, 'chromedriver.exe')

        print(f"üîç –ò—â—É chromedriver –ø–æ –ø—É—Ç–∏: {driver_path}")

        if os.path.exists(driver_path):
            print(f"‚úÖ Chromedriver –Ω–∞–π–¥–µ–Ω: {driver_path}")

            try:
                from selenium.webdriver.chrome.service import Service

                # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ —á—Ç–æ–±—ã —Å–∫—Ä—ã—Ç—å –ª–æ–≥–∏
                service = Service(executable_path=driver_path)
                service.creationflags = 0x08000000  # CREATE_NO_WINDOW

                # –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ –ª–æ–≥–æ–≤ Selenium
                import warnings
                warnings.filterwarnings("ignore")

                driver = webdriver.Chrome(service=service, options=chrome_options)
                print("‚úÖ Chrome –¥—Ä–∞–π–≤–µ—Ä —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
                return driver

            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥—Ä–∞–π–≤–µ—Ä–∞: {e}")
                print("üîÑ –ü—Ä–æ–±—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±...")

                # –ü—Ä–æ–±—É–µ–º –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –ø—É—Ç–∏
                try:
                    driver = webdriver.Chrome(options=chrome_options)
                    print("‚úÖ Chrome –¥—Ä–∞–π–≤–µ—Ä —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)")
                    return driver
                except Exception as e2:
                    print(f"‚ùå –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e2}")
                    return None
        else:
            print(f"‚ùå Chromedriver –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {driver_path}")

            # –ü—Ä–æ–±—É–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            try:
                driver = webdriver.Chrome(options=chrome_options)
                print("‚úÖ Chrome –¥—Ä–∞–π–≤–µ—Ä –Ω–∞–π–¥–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
                return driver
            except Exception as e:
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ Chrome –¥—Ä–∞–π–≤–µ—Ä: {e}")
                return None

    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ WebDriver: {e}")
        return None


# –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –Ω–µ–Ω—É–∂–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
def clean_news_text(text):
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –æ—Ç —Å–æ–≥–ª–∞—Å–∏–π, –∫—É–∫–∏ –∏ –¥—Ä—É–≥–æ–π –Ω–µ–Ω—É–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    if not text:
        return ""

    # –£–¥–∞–ª—è–µ–º —Ñ—Ä–∞–∑—ã –æ —Å–æ–≥–ª–∞—Å–∏–∏ —Å —É—Å–ª–æ–≤–∏—è–º–∏, –∫—É–∫–∞—Ö –∏ —Ç.–¥.
    patterns_to_remove = [
        r'–ü—Ä–∏–Ω–∏–º–∞—é —É—Å–ª–æ–≤–∏—è.*?\n',
        r'–°–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É.*?\n',
        r'–°–æ–≥–ª–∞—Å–µ–Ω.*?\n',
        r'–ü—Ä–∏–Ω–∏–º–∞—é.*?\n',
        r'cookies.*?\n',
        r'–∫—É–∫–∏.*?\n',
        r'–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.*?\n',
        r'–ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è.*?\n',
        r'–ü–æ–¥–ø–∏—Å–∫–∞.*?\n',
        r'–†–∞—Å—Å—ã–ª–∫–∞.*?\n',
        r'–†–µ–∫–ª–∞–º–∞.*?\n',
        r'–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥.*?\n',
        r'Ria\.ru.*AppStore.*\n?',
        r'Ria\.ru.*RuStore.*\n?',
        r'–í–µ—Ä—Å–∏—è \d{4}\.\d+.*\n?',
        r'¬© \d{4}.*\n?',
        r'–ú–ò–ê ¬´–†–æ—Å—Å–∏—è —Å–µ–≥–æ–¥–Ω—è¬ª.*\n?',
        r'–°–µ—Ç–µ–≤–æ–µ –∏–∑–¥–∞–Ω–∏–µ.*–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ.*\n?',
        r'–°–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ –æ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏.*\n?',
        r'–£—á—Ä–µ–¥–∏—Ç–µ–ª—å.*\n?',
        r'–ü—Ä–∞–≤–∏–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.*\n?',
        r'–ü—Ä–∞–≤–∏–ª–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.*\n?',
        r'–ì–ª–∞–≤–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä.*\n?',
        r'–ê–¥—Ä–µ—Å —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –ø–æ—á—Ç—ã.*\n?',
        r'–§–°77-\d+.*\n?',
        r'\d{1,2}\s+[–∞-—è]+\s+\d{4}\s+–≥–æ–¥–∞.*\n?',
    ]

    for pattern in patterns_to_remove:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É–¥–∞–ª—è–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    lines = text.split('\n')
    cleaned_lines = []

    for line in lines:
        line = line.strip()
        if not line or len(line) < 10:
            continue

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Ç—Ä–æ–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        is_technical = False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        if any(excl_word.lower() in line.lower() for excl_word in EXCLUDE_KEYWORDS):
            is_technical = True

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º
        for pattern in TECHNICAL_PATTERNS:
            if re.search(pattern, line, re.IGNORECASE):
                is_technical = True
                break

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏ –∏–ª–∏ —Å–ª—É–∂–µ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        if ('@' in line and ('.ru' in line or '.com' in line)) or '—Ç–µ–ª.' in line.lower():
            is_technical = True

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –¥–∞—Ç–æ–π –∏–ª–∏ –Ω–æ–º–µ—Ä–æ–º
        if re.match(r'^\d{1,2}[\.\/]\d{1,2}[\.\/]\d{4}', line) or re.match(r'^‚Ññ?\s*\d+', line):
            is_technical = True

        if not is_technical:
            cleaned_lines.append(line)

    return '\n'.join(cleaned_lines)


# –ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç—å—é
def is_news_text(text, min_length=15, max_length=8000):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç—å—é"""
    if not text:
        return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
    if len(text) < min_length or len(text) > max_length:
        return False

    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –±–æ–ª–µ–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
    lines = text.split('\n')
    valid_lines_count = 0

    for line in lines:
        line = line.strip()
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        is_technical_line = False
        for pattern in TECHNICAL_PATTERNS:
            if re.search(pattern, line, re.IGNORECASE):
                is_technical_line = True
                break

        if is_technical_line:
            continue

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —Å—Ç—Ä–æ–∫–µ
        line_lower = line.lower()
        news_word_count = sum(1 for keyword in NEWS_KEYWORDS if keyword.lower() in line_lower)

    # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ - —ç—Ç–æ –Ω–æ–≤–æ—Å—Ç—å
    if news_word_count >= 2:
        return True

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    technical_count = 0
    for pattern in TECHNICAL_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            technical_count += 1

    # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ - —ç—Ç–æ –Ω–µ –Ω–æ–≤–æ—Å—Ç—å
    if technical_count > 2:
        return False

    return True


def remove_duplicate_paragraphs(text):
    """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∞–±–∑–∞—Ü–µ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return ""

    paragraphs = text.split('\n\n')
    unique_paragraphs = []
    seen_hashes = set()

    for para in paragraphs:
        if not para.strip():
            continue

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        normalized = re.sub(r'\s+', ' ', para.strip().lower())

        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∏ –¥–∞—Ç—ã –¥–ª—è –ª—É—á—à–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        normalized = re.sub(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', '', normalized)
        normalized = re.sub(r'\d{1,2}:\d{2}', '', normalized)
        normalized = re.sub(r'\d{1,2}\.\d{2}\.\d{4}', '', normalized)

        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        normalized = normalized.strip()

        if not normalized or len(normalized) < 30:
            continue

        # –°–æ–∑–¥–∞–µ–º —Ö—ç—à –∏–∑ –ø–µ—Ä–≤—ã—Ö 100 —Å–∏–º–≤–æ–ª–æ–≤
        if len(normalized) > 100:
            hash_text = normalized[:100]
        else:
            hash_text = normalized

        para_hash = create_hash(hash_text)

        if para_hash not in seen_hashes:
            seen_hashes.add(para_hash)
            unique_paragraphs.append(para.strip())

    return '\n\n'.join(unique_paragraphs)


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö
def format_news_with_separators(text, source_type):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç, –¥–æ–±–∞–≤–ª—è—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –º–µ–∂–¥—É –Ω–æ–≤–æ—Å—Ç—è–º–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö"""
    if not text:
        return ""

    # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    news_items = []

    if source_type == 'TG':
        # –î–ª—è Telegram —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º —Å—Ç—Ä–æ–∫
        items = text.split('\n\n')
        for item in items:
            item = item.strip()
            if item and len(item) > 50 and is_news_text(item):
                news_items.append(item)

    elif source_type == 'VK':
        # –î–ª—è VK —Ç–∞–∫–∂–µ —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º
        items = text.split('\n\n')
        for item in items:
            item = item.strip()
            if item and len(item) > 50 and is_news_text(item):
                news_items.append(item)

    else:  # –î–ª—è –≤–µ–±-—Å–∞–π—Ç–æ–≤
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã –∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ–º
        paragraphs = text.split('\n')
        current_item = ""

        for para in paragraphs:
            para = para.strip()
            if not para:
                if current_item and len(current_item) > 50:
                    news_items.append(current_item.strip())
                    current_item = ""
                continue

            if len(para) < 100 and para and para[0].isupper() and current_item:
                if len(current_item) > 50:
                    news_items.append(current_item.strip())
                current_item = para
            else:
                if current_item:
                    current_item += "\n" + para
                else:
                    current_item = para

        if current_item and len(current_item) > 50:
            news_items.append(current_item.strip())

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö
    if not news_items:
        return ""

    formatted_parts = []
    for i, item in enumerate(news_items[:5]):  # –ù–µ –±–æ–ª–µ–µ 5 –Ω–æ–≤–æ—Å—Ç–µ–π
        formatted_parts.append(item.strip())
        if i < len(news_items[:5]) - 1:
            formatted_parts.append("&" * 40)

    return '\n'.join(formatted_parts)


# –ü–∞—Ä—Å–∏–Ω–≥ Telegram —á–µ—Ä–µ–∑ –≤–µ–±-–≤–µ—Ä—Å–∏—é
def parse_telegram_channel_web(channel_url):
    """–ü–∞—Ä—Å–∏–Ω–≥ Telegram –∫–∞–Ω–∞–ª–æ–≤ —á–µ—Ä–µ–∑ –≤–µ–±-–≤–µ—Ä—Å–∏—é —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö"""
    try:
        logger.debug(f"–ü–∞—Ä—Å–∏–Ω–≥ Telegram –∫–∞–Ω–∞–ª–∞: {channel_url}")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º username –∫–∞–Ω–∞–ª–∞ –∏–∑ URL
        username = channel_url.split('/')[-1]
        username = username.split('?')[0]

        if username.startswith('s/'):
            username = username[2:]
        if username.startswith('@'):
            username = username[1:]

        web_url = f"https://t.me/s/{username}"
        logger.debug(f"Web URL: {web_url}")

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
        }

        response = requests.get(web_url, headers=headers, timeout=15)

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            # –ò—â–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
            messages = []

            # –°–ø–æ—Å–æ–± 1: –ò—â–µ–º –ø–æ –∫–ª–∞—Å—Å—É tgme_widget_message_text
            message_divs = soup.find_all('div', class_='tgme_widget_message_text')

            if message_divs:
                logger.debug(f"–ù–∞–π–¥–µ–Ω–æ {len(message_divs)} —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ –∫–ª–∞—Å—Å—É tgme_widget_message_text")
                for msg_div in message_divs:
                    text = msg_div.get_text(separator='\n', strip=False)
                    if text and len(text.strip()) > 50:
                        messages.append(text.strip())

            # –°–ø–æ—Å–æ–± 2: –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –ø–æ –¥—Ä—É–≥–∏–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
            if not messages:
                # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º
                all_text_elements = soup.find_all(['div', 'p', 'span'])
                for element in all_text_elements:
                    text = element.get_text(separator='\n', strip=True)
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    if text and len(text) > 100 and not any(
                            excl_word in text.lower() for excl_word in EXCLUDE_KEYWORDS):
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –≤—Ä–µ–º–µ–Ω–µ–º –∏–ª–∏ –¥–∞—Ç–æ–π
                        if not re.match(r'^\d{1,2}:\d{2}$', text) and not re.match(r'^\d{1,2}\s+[–∞-—è]+', text):
                            messages.append(text)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
            news_items = []
            for msg in messages:
                # –û—á–∏—â–∞–µ–º –æ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
                msg_cleaned = clean_news_text(msg)

                if msg_cleaned and len(msg_cleaned) > 50 and is_news_text(msg_cleaned):
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
                    msg_cleaned = re.sub(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', '', msg_cleaned).strip()
                    msg_cleaned = re.sub(r'\d{1,2}:\d{2}', '', msg_cleaned).strip()
                    msg_cleaned = re.sub(r'\n{3,}', '\n\n', msg_cleaned)

                    if msg_cleaned:
                        news_items.append(msg_cleaned)

            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            unique_news = []
            seen_hashes = set()

            for news in news_items:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                normalized = re.sub(r'\s+', ' ', news.lower().strip())
                normalized = re.sub(r'[^\w\s]', '', normalized)
                if len(normalized) > 30:
                    news_hash = create_hash(normalized[:100])
                    if news_hash not in seen_hashes:
                        seen_hashes.add(news_hash)
                        unique_news.append(news)

            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö
            if unique_news:
                formatted_parts = []
                for i, news in enumerate(unique_news[:5]):  # –ë–µ—Ä–µ–º –¥–æ 5 –Ω–æ–≤–æ—Å—Ç–µ–π
                    formatted_parts.append(news.strip())
                    if i < len(unique_news[:5]) - 1:
                        formatted_parts.append("&" * 40)

                formatted_text = '\n'.join(formatted_parts)
                logger.debug(f"–£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ {len(unique_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {channel_url}")
                return formatted_text
            else:
                logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –∫–∞–Ω–∞–ª–µ {channel_url}")
                return ""

        else:
            logger.error(f"–û—à–∏–±–∫–∞ HTTP {response.status_code} –¥–ª—è {channel_url}")
            return ""

    except requests.exceptions.Timeout:
        logger.error(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ Telegram –∫–∞–Ω–∞–ª–∞ {channel_url}")
        return ""
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ Telegram –∫–∞–Ω–∞–ª–∞ {channel_url}: {e}")
        return ""


# –§—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ä–µ—Å—É—Ä—Å–æ–≤
def parse_website(url, driver=None):
    """–ü–∞—Ä—Å–∏–Ω–≥ –æ–±—ã—á–Ω—ã—Ö –≤–µ–±-—Å–∞–π—Ç–æ–≤"""
    try:
        if driver:
            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Selenium –µ—Å–ª–∏ –¥—Ä–∞–π–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω
            try:
                driver.get(url)
                from selenium.webdriver.support.ui import WebDriverWait
                from selenium.webdriver.support import expected_conditions as EC
                from selenium.webdriver.common.by import By

                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                page_source = driver.page_source
            except Exception as selenium_error:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Selenium –¥–ª—è {url}: {selenium_error}")
                # –ü—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ requests
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                response = requests.get(url, headers=headers, timeout=10)
                response.raise_for_status()
                page_source = response.text
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º requests –µ—Å–ª–∏ –Ω–µ—Ç –¥—Ä–∞–π–≤–µ—Ä–∞
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            page_source = response.text

        soup = BeautifulSoup(page_source, 'html.parser')

        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for script in soup(["script", "style", "nav", "footer", "header", "aside", "form"]):
            script.decompose()

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç
        text = soup.get_text(separator='\n', strip=True)

        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = '\n'.join(chunk for chunk in chunks if chunk)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –Ω–æ–≤–æ—Å—Ç–µ–π
        text = clean_news_text(text)

        return text[:5000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å–∞–π—Ç–∞ {url}: {e}")
        return ""


def parse_vk_group(group_url):
    """–ü–∞—Ä—Å–∏–Ω–≥ VK –≥—Ä—É–ø–ø —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö"""
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–±–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é –¥–ª—è –æ–±—Ö–æ–¥–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
        if 'm.vk.com' not in group_url:
            mobile_url = group_url.replace('vk.com', 'm.vk.com')
        else:
            mobile_url = group_url

        headers = {
            'User-Agent': 'Mozilla/5.0 (Linux; Android 10) AppleWebKit/537.36'
        }

        response = requests.get(mobile_url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # –ü–æ–∏—Å–∫ –ø–æ—Å—Ç–æ–≤
        posts = []

        # –ò—â–µ–º –ø–æ —Ä–∞–∑–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
        selectors = [
            'div.wall_item',
            'div.wi_body',
            'div.post_content',
            'div.wall_post_text',
            'div.post_text'
        ]

        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                for element in elements:
                    post_text = element.get_text(separator='\n', strip=True)
                    if post_text and len(post_text) > 100:
                        posts.append(post_text)

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º, –∏—â–µ–º –ª—é–±–æ–π –∑–Ω–∞—á–∏–º—ã–π —Ç–µ–∫—Å—Ç
        if not posts:
            for div in soup.find_all('div'):
                text = div.get_text(separator='\n', strip=True)
                if text and 200 < len(text) < 2000:
                    posts.append(text)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ—Å—Ç—ã
        news_items = []
        for post in posts:
            post_cleaned = clean_news_text(post)
            if post_cleaned and len(post_cleaned) > 50 and is_news_text(post_cleaned):
                news_items.append(post_cleaned)

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_news = []
        seen_hashes = set()

        for news in news_items:
            normalized = re.sub(r'\s+', ' ', news.lower().strip())
            normalized = re.sub(r'[^\w\s]', '', normalized)
            if len(normalized) > 30:
                news_hash = create_hash(normalized[:100])
                if news_hash not in seen_hashes:
                    seen_hashes.add(news_hash)
                    unique_news.append(news)

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö
        if unique_news:
            formatted_parts = []
            for i, news in enumerate(unique_news[:5]):  # –ù–µ –±–æ–ª–µ–µ 5 –Ω–æ–≤–æ—Å—Ç–µ–π
                formatted_parts.append(news.strip())
                if i < len(unique_news[:5]) - 1:
                    formatted_parts.append("&" * 40)

            return '\n'.join(formatted_parts)
        else:
            return ""

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ VK –≥—Ä—É–ø–ø—ã {group_url}: {e}")
        return ""


# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞
def parse_source(row, driver=None):
    """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
    name = row['–ù–∞–∑–≤–∞–Ω–∏–µ']
    url = row['–°—Å—ã–ª–∫–∞']
    source_type = row['–¢–∏–ø']
    source_id = row['id']

    logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥: {name} ({source_type})")

    text = ""

    try:
        if source_type == 'TG':
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ–±-–ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è Telegram
            text = parse_telegram_channel_web(url)
        elif source_type == 'VK':
            text = parse_vk_group(url)
        else:
            # –î–ª—è –≤–µ–±-—Å–∞–π—Ç–æ–≤ –ø–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –µ–≥–æ
            raw_text = parse_website(url, driver)
            if raw_text:
                text = format_news_with_separators(raw_text, source_type)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        if text:
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            lines = text.split('\n')
            cleaned_lines = []

            for line in lines:
                line = line.strip()
                if line or line == "&" * 40:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –∏ –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                    cleaned_lines.append(line)

            text = '\n'.join(cleaned_lines)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        if text:
            save_parsing_result_to_db(source_id, text)

        return {
            'id': source_id,
            'name': name,
            'url': url,
            'type': source_type,
            'level': row.get('–£—Ä–æ–≤–µ–Ω—å', ''),
            'theme': row.get('–¢–µ–º–∞—Ç–∏–∫–∞', ''),
            'text': text[:10000] if text else "",
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {name}: {e}")
        return None


# –§—É–Ω–∫—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
def save_results(results, filename='../data/ekb_news.txt'):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –ò –≤ JSON"""
    try:
        # ====== –°–¢–ê–†–´–ô –ö–û–î (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ txt) ======
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"–û–ë–ù–û–í–õ–ï–ù–û: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n\n")

            sources_with_news = 0
            for result in results:
                if result and result.get('text') and result['text'].strip():
                    sources_with_news += 1

                    f.write(f"–°–°–´–õ–ö–ê: {result['url']}\n")
                    f.write(f"–í–†–ï–ú–Ø –ü–ê–†–°–ò–ù–ì–ê: {result['timestamp']}\n")
                    f.write("-" * 40 + "\n")

                    f.write(result['text'] + "\n")
                    f.write("=" * 80 + "\n\n")

            if sources_with_news == 0:
                f.write("–ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —ç—Ç–æ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏\n\n")

        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏: {sources_with_news}")

        # ====== –ù–û–í–´–ô –ö–û–î (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º JSON) ======
        json_filename = filename.replace('.txt', '_structured.json')
        structured_news = []

        for result in results:
            if result and result.get('text') and result['text'].strip():
                # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
                news_items = result['text'].split("&" * 40)

                for news_text in news_items:
                    if news_text.strip():
                        structured_news.append({
                            "source_name": result['name'],
                            "source_url": result['url'],
                            "source_type": result['type'],
                            "parse_time": result['timestamp'],
                            "raw_text": news_text.strip(),
                            # –≠—Ç–∏ –ø–æ–ª—è –∑–∞–ø–æ–ª–Ω–∏—Ç AI –ø–æ–∑–∂–µ
                            "summary": "",
                            "category": "",
                            "criticality": 0,
                            "sentiment": "",
                            "ai_analyzed": False,
                            "analyzed_at": None
                        })

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump({
                "version": "1.0",
                "generated_at": datetime.now().isoformat(),
                "total_news": len(structured_news),
                "sources_count": sources_with_news,
                "news": structured_news
            }, f, ensure_ascii=False, indent=2)

        logger.info(f"–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {json_filename}")
        logger.info(f"–°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(structured_news)}")

        return structured_news  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ AI

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
        return []


# –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–∞—Ä—Å–∏–Ω–≥–∞
def main_loop():
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º 10 –º–∏–Ω—É—Ç"""

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥—Ä–∞–π–≤–µ—Ä–∞
    driver = None

    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        # init_database()

        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ Excel
        # if import_from_excel_to_db():
        #     logger.info("–ò—Å—Ç–æ—á–Ω–∏–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
        # else:
        #     logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ Excel. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤ –ë–î.")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Selenium –° –ü–†–ê–í–ò–õ–¨–ù–´–ú –ü–£–¢–ï–ú
        try:
            driver = init_webdriver()
            if driver:
                logger.info("‚úÖ Selenium WebDriver –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            else:
                logger.warning("‚ö†Ô∏è Selenium WebDriver –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, —Ä–∞–±–æ—Ç–∞–µ–º –±–µ–∑ –Ω–µ–≥–æ")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Selenium: {e}")
            logger.warning("‚ö†Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É –±–µ–∑ Selenium")
            driver = None

        while True:
            try:
                logger.info("–ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —Ü–∏–∫–ª –ø–∞—Ä—Å–∏–Ω–≥–∞...")

                # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
                sources = get_sources_from_db()
                if not sources:
                    logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
                    time.sleep(600)
                    continue

                # –ü–∞—Ä—Å–∏–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                results = []
                for row in sources:
                    # –ü–µ—Ä–µ–¥–∞–µ–º –¥—Ä–∞–π–≤–µ—Ä, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω None
                    result = parse_source(row, driver)
                    if result:
                        results.append(result)
                    # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    time.sleep(2)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                save_results(results)

                # –ó–∞–ø—É—Å–∫–∞–µ–º AI –∞–Ω–∞–ª–∏–∑
                try:
                    load_dotenv()
                    auth_key = os.getenv('AUTH_KEY')
                    if auth_key:
                        logger.info("ü§ñ –ó–∞–ø—É—Å–∫–∞—é AI –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π...")
                        gigachat_analysis.start_analysis('../data/ekb_news.txt', auth_key)
                    else:
                        logger.warning("‚ö†Ô∏è –ö–ª—é—á GigaChat –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞—é AI –∞–Ω–∞–ª–∏–∑")
                except Exception as ai_error:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ AI –∞–Ω–∞–ª–∏–∑–∞: {ai_error}")

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑—ã –≤ –ë–î —á–µ—Ä–µ–∑ integration_layer
                try:
                    import json
                    json_file = '../data/ekb_news_analyzed.json'
                    if os.path.exists(json_file):
                        with open(json_file, 'r', encoding='utf-8') as f:
                            analysis_data = json.load(f)

                        if analysis_data.get('status') == 'success':
                            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö –∏–º–ø–æ—Ä—Ç–æ–≤
                            try:
                                # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ integration_layer
                                import sys
                                import os
                                current_dir = os.path.dirname(os.path.abspath(__file__))
                                project_root = os.path.dirname(os.path.dirname(current_dir))
                                sys.path.insert(0, project_root)

                                from integration_layer import process_and_save_news
                                processed = process_and_save_news()
                                logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã: {processed} –Ω–æ–≤–æ—Å—Ç–µ–π")
                            except ImportError:
                                logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å integration_layer")
                            except Exception as e:
                                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞–Ω–∞–ª–∏–∑–æ–≤: {e}")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–æ–≤: {e}")

                logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(results)}")
                logger.info("‚è≥ –°–ª–µ–¥—É—é—â–∏–π –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ —á–∞—Å...")
                # –ñ–¥–µ–º —á–∞—Å
                time.sleep(3600)

            except KeyboardInterrupt:
                logger.info("üõë –ü–∞—Ä—Å–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                break
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
                time.sleep(300)  # –ñ–¥–µ–º 5 –º–∏–Ω—É—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ

    finally:
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –¥—Ä–∞–π–≤–µ—Ä, –µ—Å–ª–∏ –æ–Ω –±—ã–ª —Å–æ–∑–¥–∞–Ω
        if driver:
            try:
                driver.quit()
                logger.info("üîå Selenium WebDriver –∑–∞–∫—Ä—ã—Ç")
            except:
                pass


def is_municipal_problem(text):
    """–§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –º—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã"""
    if not text or len(text) < 50:
        return False

    text_lower = text.lower()

    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ú–£–ù–ò–¶–ò–ü–ê–õ–¨–ù–´–• –ø—Ä–æ–±–ª–µ–º
    problem_keywords = [
        '–∞–≤–∞—Ä–∏—è', '–ø—Ä–æ—Ä—ã–≤', '–∑–∞—Ç–æ–ø–ª–µ–Ω–∏–µ', '–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ', '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç',
        '—Å–≤–∞–ª–∫–∞', '–º—É—Å–æ—Ä', '—è–º–∞', '–¥–æ—Ä–æ–≥', '—Å–≤–µ—Ç–æ—Ñ–æ—Ä', '–ª–∏—Ñ—Ç',
        '–æ—Ç–æ–ø–ª–µ–Ω–∏–µ', '–≤–æ–¥–∞', '—Å–≤–µ—Ç', '—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ', '—Ç–µ–ø–ª–æ',
        '–∂–∞–ª–æ–±–∞', '–æ–±—Ä–∞—â–µ–Ω–∏–µ', '–ø—Ä–æ–±–ª–µ–º–∞', '–∏–Ω—Ü–∏–¥–µ–Ω—Ç', '–î–¢–ü',
        '—É–±–æ—Ä–∫–∞', '–±–ª–∞–≥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ', '–ñ–ö–•', '–∫–æ–º–º—É–Ω–∞–ª–∫–∞', '–ø–æ–¥–≤–∞–ª',
        '–∫—Ä—ã—à–∞', '—Ç—Ä—É–±–∞', '–∫–∞–Ω–∞–ª–∏–∑–∞—Ü–∏—è', '—É—Ç–µ—á–∫–∞', '–∑–∞—Å–æ—Ä'
    ]

    # –°–ª–æ–≤–∞-—Ñ–∏–ª—å—Ç—Ä—ã (—á—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º)
    skip_keywords = [
        '–Ω–æ–≤–æ—Å—Ç—å', '–∞–Ω–æ–Ω—Å', '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ', '—Ñ–µ—Å—Ç–∏–≤–∞–ª—å', '–∫–æ–Ω—Ü–µ—Ä—Ç',
        '–≤—ã—Å—Ç–∞–≤–∫–∞', '–æ—Ç–∫—Ä—ã—Ç–∏–µ', '–ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ', '–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ',
        '—Ä–µ–∫–ª–∞–º–∞', '–∞–∫—Ü–∏—è', '—Å–∫–∏–¥–∫–∞', '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞', '—á–µ–±—É—Ä–∞—à–∫–∞',
        '–∏–≥—Ä—É—à–∫–∞', '–∫–æ–ª–ª–µ–∫—Ü–∏—è', '–∫–∏–Ω–æ', '—Ñ–∏–ª—å–º', '–ø—Ä–µ–º—å–µ—Ä–∞'
    ]

    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –µ—Å—Ç—å skip-—Å–ª–æ–≤–∞
    if any(skip_word in text_lower for skip_word in skip_keywords):
        return False

    # –°—á–∏—Ç–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–ª–æ–≤–∞
    problem_count = sum(1 for word in problem_keywords if word in text_lower)

    return problem_count >= 2


if __name__ == "__main__":
    print("–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥–∞")
    print("=" * 60)
    print(" –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ —Å Selenium –∏ –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö...")
    main_loop()
