import sys
import os
import json
import requests
import logging
import sqlite3
import time
import uuid
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

# ========== –ò–ú–ü–û–†–¢ NEURAL NETWORK ==========
print(f"\nüîç –ò–ú–ü–û–†–¢ NEURAL NETWORK")

# 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ neural_network
current_file = os.path.abspath(__file__)  # .../back/integration_layer.py
back_dir = os.path.dirname(current_file)  # .../back
backend_dir = os.path.dirname(back_dir)  # .../backend
neural_network_dir = os.path.join(backend_dir, 'neural_network')

print(f"üìÅ back_dir: {back_dir}")
print(f"üìÅ backend_dir: {backend_dir}")
print(f"üìÅ neural_network_dir: {neural_network_dir}")

# 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø–∞–ø–∫–∞
if not os.path.exists(neural_network_dir):
    print(f"‚ùå –ü–∞–ø–∫–∞ neural_network –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
    print(f"   –ò—â–µ–º –≤ –¥—Ä—É–≥–∏—Ö –º–µ—Å—Ç–∞—Ö...")

    # –ò—â–µ–º –≤ —Ä–∞–¥–∏—É—Å–µ 3 —É—Ä–æ–≤–Ω–µ–π
    for root, dirs, files in os.walk(backend_dir, topdown=True):
        if 'neural_network' in dirs:
            neural_network_dir = os.path.join(root, 'neural_network')
            print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø—É—Ç—å: {neural_network_dir}")
            break

# 3. –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
neural_network_file = os.path.join(neural_network_dir, 'neural_network.py')
print(f"üìÑ –§–∞–π–ª neural_network.py: {neural_network_file}")
print(f"üìÇ –°—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(neural_network_file)}")

# 4. –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º
analyze_news_article = None

if os.path.exists(neural_network_file):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location("neural_network", neural_network_file)
        neural_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(neural_module)

        if hasattr(neural_module, 'analyze_news_article'):
            analyze_news_article = neural_module.analyze_news_article
            print("‚úÖ –§—É–Ω–∫—Ü–∏—è analyze_news_article –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞!")
        else:
            print("‚ö†Ô∏è –§—É–Ω–∫—Ü–∏—è analyze_news_article –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –º–æ–¥—É–ª–µ")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
        import traceback

        traceback.print_exc()
else:
    print("‚ùå –§–∞–π–ª neural_network.py –Ω–µ –Ω–∞–π–¥–µ–Ω!")

# 5. –ï—Å–ª–∏ –∏–º–ø–æ—Ä—Ç –Ω–µ —É–¥–∞–ª—Å—è - —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
if analyze_news_article is None:
    print("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É—é –∑–∞–≥–ª—É—à–∫—É –¥–ª—è analyze_news_article")


    def analyze_news_article(text, source_url="", source_name="", parse_time=""):
        print(f"[–ó–ê–ì–õ–£–®–ö–ê] analyze_news_article: {text[:50]}...")
        return [{
            "category": "–î—Ä—É–≥–æ–µ",
            "criticality": 0,
            "sentiment": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è",
            "original_preview": text[:150] + "...",
            "street": "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥"
        }]

print("‚úÖ –ò–º–ø–æ—Ä—Ç neural network –∑–∞–≤–µ—Ä—à–µ–Ω\n")


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# ========== –§–£–ù–ö–¶–ò–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–ò –ú–£–ù–ò–¶–ò–ü–ê–õ–¨–ù–û–ì–û –ö–û–ù–¢–ï–ù–¢–ê ==========
def is_municipal_problem(text):
    """–í–°–ï–ì–î–ê –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç True - –ø—Ä–∏–Ω–∏–º–∞–µ–º –í–°–ï –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
    if not text or len(text) < 30:
        print(f"      ‚è≠Ô∏è –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
        return False

    text_lower = text.lower()

    # –°—á–∏—Ç–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    municipal_keywords = [
        '–∞–≤–∞—Ä–∏—è', '–ø—Ä–æ—Ä—ã–≤', '–∑–∞—Ç–æ–ø–ª–µ–Ω–∏–µ', '–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ', '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç',
        '—Å–≤–∞–ª–∫–∞', '–º—É—Å–æ—Ä', '—è–º–∞', '–¥–æ—Ä–æ–≥', '—Å–≤–µ—Ç–æ—Ñ–æ—Ä', '–ª–∏—Ñ—Ç',
        '–æ—Ç–æ–ø–ª–µ–Ω–∏–µ', '–≤–æ–¥–∞', '—Å–≤–µ—Ç', '—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ', '—Ç–µ–ø–ª–æ',
        '–∂–∞–ª–æ–±–∞', '–æ–±—Ä–∞—â–µ–Ω–∏–µ', '–ø—Ä–æ–±–ª–µ–º–∞', '–∏–Ω—Ü–∏–¥–µ–Ω—Ç', '–î–¢–ü',
        '—É–±–æ—Ä–∫–∞', '–±–ª–∞–≥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ', '–ñ–ö–•', '–∫–æ–º–º—É–Ω–∞–ª–∫–∞'
    ]

    commercial_keywords = [
        '–∞–∫—Ü–∏—è', '—Å–∫–∏–¥–∫–∞', '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞', '–æ—Ç–∫—Ä—ã—Ç–∏–µ', '–∑–∞–ø—É—Å–∫',
        '–Ω–æ–≤–∏–Ω–∫–∞', '–∫–æ–ª–ª–µ–∫—Ü–∏—è', '–∏–≥—Ä—É—à–∫–∞', '–º–∞–≥–∞–∑–∏–Ω', '—Ä–µ—Å—Ç–æ—Ä–∞–Ω',
        '–ø—Ä–æ–¥—É–∫—Ç', '—É—Å–ª—É–≥–∞', '—Ü–µ–Ω–∞', '–∫—É–ø–∏—Ç—å', '–∑–∞–∫–∞–∑'
    ]

    event_keywords = [
        '—Ñ–µ—Å—Ç–∏–≤–∞–ª—å', '–∫–æ–Ω—Ü–µ—Ä—Ç', '–≤—ã—Å—Ç–∞–≤–∫–∞', '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ',
        '–ø—Ä–∞–∑–¥–Ω–∏–∫', '—Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ', '—Ç—É—Ä–Ω–∏—Ä', '—à–æ—É', '—Å–ø–µ–∫—Ç–∞–∫–ª—å'
    ]

    # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
    municipal_count = sum(1 for word in municipal_keywords if word in text_lower)
    commercial_count = sum(1 for word in commercial_keywords if word in text_lower)
    event_count = sum(1 for word in event_keywords if word in text_lower)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    if municipal_count >= 2:
        content_type = "–ú–£–ù–ò–¶–ò–ü–ê–õ–¨–ù–ê–Ø –ø—Ä–æ–±–ª–µ–º–∞"
    elif commercial_count >= 2:
        content_type = "–ö–û–ú–ú–ï–†–ß–ï–°–ö–ê–Ø –Ω–æ–≤–æ—Å—Ç—å"
    elif event_count >= 1:
        content_type = "–ú–ï–†–û–ü–†–ò–Ø–¢–ò–ï"
    else:
        content_type = "–û–ë–©–ê–Ø –Ω–æ–≤–æ—Å—Ç—å"

    print(f"      üìä –¢–∏–ø: {content_type}")
    print(f"      üìù –ú—É–Ω–∏—Ü. —Å–ª–æ–≤–∞: {municipal_count}, –ö–æ–º–º–µ—Ä—á.: {commercial_count}, –°–æ–±—ã—Ç–∏—è: {event_count}")

    # –í–°–ï–ì–î–ê –≤–æ–∑–≤—Ä–∞—â–∞–µ–º True (–ø—Ä–∏–Ω–∏–º–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏)
    return True


# ========== –§–£–ù–ö–¶–ò–Ø –í–ê–õ–ò–î–ê–¶–ò–ò –û–¢–í–ï–¢–ê –ò–ò ==========
def process_ai_response(ai_result, source_url="", parse_time="", news_text=""):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –ò–ò - –†–ê–ë–û–ß–ê–Ø –í–ï–†–°–ò–Ø"""
    try:
        if not ai_result or not isinstance(ai_result, list):
            print("      ‚ùå –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò")
            return None

        result = ai_result[0]

        if 'category' not in result or result.get('category') is None:
            print("      ‚ö†Ô∏è –ù–µ–π—Ä–æ—Å–µ—Ç—å –≤–µ—Ä–Ω—É–ª–∞ None –≤–º–µ—Å—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏")
            # –°–æ–∑–¥–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ —Ç–∏–ø—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            result = {
                "category": "–ù–æ–≤–æ—Å—Ç–∏",
                "criticality": 0,
                "sentiment": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è",
                "original_preview": "–û–±—â–∞—è –Ω–æ–≤–æ—Å—Ç—å"
            }

            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É...
        criticality = result.get('criticality', 0)
        category = result.get('category', '–î—Ä—É–≥–æ–µ')

        # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è None - –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ "–î—Ä—É–≥–æ–µ"
        if category is None:
            category = "–î—Ä—É–≥–æ–µ"

        # ‚ö†Ô∏è –î–ï–ë–ê–ì: –ø–æ–∫–∞–∂–µ–º —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏
        print(f"      üìã –ö–ª—é—á–∏ –æ—Ç–≤–µ—Ç–∞ –ò–ò: {list(result.keys())}")

        # –ê–î–ê–ü–¢–ò–†–£–ï–ú–°–Ø –ö –§–û–†–ú–ê–¢–£ –ù–ï–ô–†–û–°–ï–¢–ò
        # 1. criticality ‚Üí priority
        priority = result.get('criticality', 0)
        if priority is None:
            priority = 0

        # 2. category (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)
        category = result.get('category', '–î—Ä—É–≥–æ–µ')

        # 3. summary
        summary = result.get('original_preview', '')
        if not summary:
            summary = result.get('problem_type', '')

        # 4. location (street –∏–ª–∏ house)
        location = result.get('street', '–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥')
        if location == '—É–ª. –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è' and result.get('house'):
            location = f"{location}, {result['house']}"

        # 5. sentiment (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º)
        sentiment_ru = result.get('sentiment', '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è')
        if '–Ω–µ–≥–∞—Ç–∏–≤' in sentiment_ru:
            sentiment = 'negative'
        elif '–ø–æ–∑–∏—Ç–∏–≤' in sentiment_ru:
            sentiment = 'positive'
        else:
            sentiment = 'neutral'

        # –§–û–†–ú–ò–†–£–ï–ú –î–ê–ù–ù–´–ï
        data = {
            "text": summary[:500] if summary else news_text[:200],
            "category": category,
            "location": location,
            "sentiment": sentiment,
            "priority": int(priority),
            "metadata": json.dumps({
                "source_url": source_url,
                "parse_time": parse_time,
                "ai_response": result,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –í–ï–°–¨ –æ—Ç–≤–µ—Ç
                "processed_at": datetime.now().isoformat()
            }, ensure_ascii=False)
        }

        print(f"      ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {category} (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {priority})")
        return data

    except Exception as e:
        print(f"      ‚ùå –û—à–∏–±–∫–∞ process_ai_response: {e}")
        import traceback
        traceback.print_exc()
        return None


def extract_location(text):
    """–ò–∑–≤–ª–µ–∫–∞–µ–º –ª–æ–∫–∞—Ü–∏—é –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥"

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–¥—Ä–µ—Å–æ–≤
    patterns = [
        r'—É–ª\.\s*[\w\s\.\-]+(?=\s|$)',
        r'–ø—Ä–æ—Å–ø–µ–∫—Ç\s*[\w\s\.\-]+',
        r'–ø—Ä\.\s*[\w\s\.\-]+',
        r'—Ä–∞–π–æ–Ω\s*[\w\s\.\-]+',
        r'–º–∫—Ä–Ω\.\s*[\w\s\.\-]+',
        r'–¥–æ–º\s*\d+',
        r'–¥\.\s*\d+',
        r'–≤–æ–∑–ª–µ\s*[\w\s\.\-]+',
        r'–Ω–∞\s*[\w\s]+—É–ª–∏—Ü–µ',
        r'–≤\s*—Ä–∞–π–æ–Ω–µ\s*[\w\s\.\-]+'
    ]

    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        if matches:
            return matches[0]

    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ —Ä–∞–π–æ–Ω–æ–≤ –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥–∞
    districts = [
        '–í–µ—Ä—Ö-–ò—Å–µ—Ç—Å–∫–∏–π', '–ñ–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω—ã–π', '–ö–∏—Ä–æ–≤—Å–∫–∏–π', '–õ–µ–Ω–∏–Ω—Å–∫–∏–π',
        '–û–∫—Ç—è–±—Ä—å—Å–∫–∏–π', '–û—Ä–¥–∂–æ–Ω–∏–∫–∏–¥–∑–µ–≤—Å–∫–∏–π', '–ß–∫–∞–ª–æ–≤—Å–∫–∏–π', '–£—Ä–∞–ª–º–∞—à',
        '–≠–ª—å–º–∞—à', '–í–ò–ó', '–¶–µ–Ω—Ç—Ä', '–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π', '–ë–æ—Ç–∞–Ω–∏–∫–∞'
    ]

    for district in districts:
        if district.lower() in text.lower():
            return f"{district} —Ä–∞–π–æ–Ω"

    return "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥"


# ========== –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ë–ê–ó–´ –î–ê–ù–ù–´–• ==========
def init_database():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    try:
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤ –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞
        reports_dir = os.path.join(backend_dir, 'reports')
        os.makedirs(reports_dir, exist_ok=True)
        print(f"‚úÖ –ü–∞–ø–∫–∞ reports —Å–æ–∑–¥–∞–Ω–∞/–ø—Ä–æ–≤–µ—Ä–µ–Ω–∞: {reports_dir}")

        # –ü—É—Ç—å –∫ –ë–î –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞
        db_path = os.path.join(backend_dir, 'data', 'municipal_monitoring.db')
        print(f"üìÅ –ü—É—Ç—å –∫ –ë–î: {db_path}")

        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS problems (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                text TEXT NOT NULL,
                category TEXT,
                location TEXT,
                sentiment TEXT,
                priority INTEGER DEFAULT 0,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                cluster_id INTEGER,
                is_incident BOOLEAN DEFAULT 0
            )
        ''')

        # –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø —Ç–∞–±–ª–∏—Ü–∞ clusters
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS clusters (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                category TEXT,
                location TEXT,
                frequency INTEGER DEFAULT 0,
                severity INTEGER DEFAULT 1,
                example_problems TEXT,  -- –ü–†–ê–í–ò–õ–¨–ù–û–ï –ò–ú–Ø –ö–û–õ–û–ù–ö–ò
                first_seen TIMESTAMP,
                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        conn.commit()
        conn.close()
        logger.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ë–î: {e}")
        raise


# ========== –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –û–¢–ü–†–ê–í–ö–ò –í –ë–≠–ö–ï–ù–î ==========
def send_to_backend(analysis_result, source_url="", parse_time=""):
    """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –±—ç–∫–µ–Ω–¥"""
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≤ int
        priority = analysis_result.get("priority", 0)
        if priority is None:
            priority = 0
        elif isinstance(priority, str):
            try:
                priority = int(priority)
            except:
                priority = 0

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º sentiment
        sentiment = analysis_result.get("sentiment", "neutral")
        if sentiment is None:
            sentiment = "neutral"

        # –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ
        data_to_send = {
            "text": analysis_result.get("text", analysis_result.get("summary", ""))[:500],
            # –ò–ó–ú–ï–ù–ï–ù–û: –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ–±–∞ –ø–æ–ª—è
            "category": analysis_result.get("category", "–î—Ä—É–≥–æ–µ"),
            "location": analysis_result.get("location", "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥"),
            "sentiment": sentiment,
            "priority": priority,
            "metadata": analysis_result.get("metadata", json.dumps({}))  # –ò–ó–ú–ï–ù–ï–ù–û: –±–µ—Ä–µ–º –≥–æ—Ç–æ–≤—ã–π metadata
        }

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è "–ù–æ–≤–æ—Å—Ç–∏" –∏–ª–∏ "–î—Ä—É–≥–æ–µ"
        if data_to_send["category"] in ["–ù–æ–≤–æ—Å—Ç–∏", "–î—Ä—É–≥–æ–µ", "–ù–æ–≤–æ—Å—Ç—å"]:
            logger.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º: {data_to_send['category']}")
            return False

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –±—ç–∫–µ–Ω–¥ API
        backend_url = "http://localhost:8000/api/system_report"
        response = requests.post(backend_url, json=data_to_send, timeout=10)

        if response.status_code == 200:
            logger.info(f"‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ –±—ç–∫–µ–Ω–¥: {data_to_send['category']} (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {priority})")
            return True
        else:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ {response.status_code}: {response.text[:100]}")
            return False

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ send_to_backend: {e}")
        return False


# ========== –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –°–û–ó–î–ê–ù–ò–Ø –ö–õ–ê–°–¢–ï–†–û–í ==========
def create_clusters_from_problems():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–æ–±–ª–µ–º"""
    try:
        db_path = os.path.join(backend_dir, 'data', 'municipal_monitoring.db')
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        cursor.execute('''
            SELECT category, COUNT(*) as count, GROUP_CONCAT(text, ' || ') as examples
            FROM problems 
            WHERE created_at > datetime('now', '-7 days')
            GROUP BY category
            HAVING COUNT(*) > 1
        ''')

        clusters = []
        rows = cursor.fetchall()

        for row in rows:
            category = row[0]
            frequency = row[1]
            examples = row[2]

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å
            if frequency >= 5:
                severity = 3
            elif frequency >= 3:
                severity = 2
            else:
                severity = 1

            description = f"{frequency} –ø—Ä–æ–±–ª–µ–º –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}'"
            cluster_id = f"cluster_{category.lower()}_{int(time.time())}"

            cursor.execute('''
                INSERT OR REPLACE INTO clusters (id, category, description, severity, frequency, example_problems)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (cluster_id, category, description, severity, frequency, examples))

            clusters.append({
                "id": cluster_id,
                "category": category,
                "description": description,
                "severity": severity,
                "frequency": frequency
            })

        conn.commit()
        conn.close()

        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(clusters)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
        return clusters

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {e}")
        return []


def cluster_similar_problems():
    """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å—Ö–æ–∂–∏—Ö –ø—Ä–æ–±–ª–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—é"""
    try:
        db_path = os.path.join(backend_dir, 'data', 'municipal_monitoring.db')
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–æ–±–ª–µ–º—ã —Å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π –∏ –ª–æ–∫–∞—Ü–∏–µ–π
        cursor.execute('''
            SELECT category, location, COUNT(*) as frequency,
                   GROUP_CONCAT(text, ' || ') as examples,
                   MIN(created_at) as first_seen,
                   MAX(created_at) as last_seen
            FROM problems 
            WHERE created_at > datetime('now', '-7 days')
            GROUP BY category, location
            HAVING COUNT(*) > 1
            ORDER BY COUNT(*) DESC
        ''')

        clusters = []
        for row in cursor.fetchall():
            cluster = {
                "id": f"cluster_{hash(row[0] + str(row[1]))}",
                "category": row[0],
                "location": row[1] if row[1] else "–ù–µ —É–∫–∞–∑–∞–Ω–æ",
                "frequency": row[2],
                "examples": row[3].split(' || ')[:3] if row[3] else [],
                "first_seen": row[4],
                "last_seen": row[5],
                "severity": min(3, row[2] // 2 + 1)  # 1-3 –ø–æ —á–∞—Å—Ç–æ—Ç–µ
            }
            clusters.append(cluster)

        conn.close()
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(clusters)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—é")
        return clusters

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        return []


# ========== –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ó–ê–ì–†–£–ó–ö–ò –ò –û–ë–†–ê–ë–û–¢–ö–ò –ù–û–í–û–°–¢–ï–ô ==========
def process_and_save_news():
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î –¥–ª—è –¥–∞—à–±–æ—Ä–¥–∞"""
    try:
        print(f"\nüîç [process_and_save_news] –ù–ê–ß–ê–õ–û –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π")

        # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
        news_file = os.path.join(backend_dir, 'data', 'ekb_news.txt')

        print(f"   üìÅ –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É: {news_file}")
        print(f"   üìÇ –§–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(news_file)}")

        if not os.path.exists(news_file):
            print(f"‚ùå –§–∞–π–ª –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {news_file}")
            return 0

        with open(news_file, 'r', encoding='utf-8') as f:
            content = f.read()

        print(f"   üìÑ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")

        if not content:
            print("‚ùå –§–∞–π–ª –Ω–æ–≤–æ—Å—Ç–µ–π –ø—É—Å—Ç")
            return 0

        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º
        news_sections = content.split("=" * 80)
        print(f"   üì∞ –ù–∞–π–¥–µ–Ω–æ —Å–µ–∫—Ü–∏–π –Ω–æ–≤–æ—Å—Ç–µ–π: {len(news_sections)}")

        processed_count = 0

        for i, section in enumerate(news_sections):
            if not section.strip():
                continue

            print(f"\n   üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å–µ–∫—Ü–∏—é #{i + 1}")

            # –ò—â–µ–º —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            lines = section.strip().split('\n')
            news_text = ""
            source_url = ""
            parse_time = ""

            in_news = False

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                if line.startswith("–°–°–´–õ–ö–ê:"):
                    source_url = line.replace("–°–°–´–õ–ö–ê:", "").strip()
                    print(f"      üîó –ò—Å—Ç–æ—á–Ω–∏–∫: {source_url[:50]}...")
                    continue
                elif "–í–†–ï–ú–Ø –ü–ê–†–°–ò–ù–ì–ê:" in line:
                    parse_time = line.replace("–í–†–ï–ú–Ø –ü–ê–†–°–ò–ù–ì–ê:", "").strip()
                    print(f"      üïê –í—Ä–µ–º—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {parse_time}")
                    continue
                elif "-" * 40 in line:
                    in_news = True
                    continue
                elif in_news and line:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
                    if not line.startswith("=") and not line.startswith("&"):
                        news_text += line + " "

            news_text = news_text.strip()

            if news_text and len(news_text) > 50:
                print(f"      üìù –¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ ({len(news_text)} —Å–∏–º–≤–æ–ª–æ–≤):")
                print(f"      {news_text[:100]}...")

                # ============= –ò–ó–ú–ï–ù–ï–ù–ò–ï: –£–ë–ò–†–ê–ï–ú –§–ò–õ–¨–¢–†–ê–¶–ò–Æ =============
                # –í–º–µ—Å—Ç–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                text_lower = news_text.lower()

                municipal_keywords = ['–∞–≤–∞—Ä–∏—è', '–ø—Ä–æ—Ä—ã–≤', '–∑–∞—Ç–æ–ø–ª–µ–Ω–∏–µ', '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç', '—Å–≤–∞–ª–∫–∞', '–º—É—Å–æ—Ä', '—è–º–∞',
                                      '–¥–æ—Ä–æ–≥', '—Å–≤–µ—Ç–æ—Ñ–æ—Ä', '–æ—Ç–æ–ø–ª–µ–Ω–∏–µ']
                commercial_keywords = ['–∞–∫—Ü–∏—è', '—Å–∫–∏–¥–∫–∞', '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞', '–æ—Ç–∫—Ä—ã—Ç–∏–µ', '–∑–∞–ø—É—Å–∫', '–∫–æ–ª–ª–µ–∫—Ü–∏—è', '–∏–≥—Ä—É—à–∫–∞',
                                       '–º–∞–≥–∞–∑–∏–Ω', '—Ä–µ—Å—Ç–æ—Ä–∞–Ω']
                event_keywords = ['—Ñ–µ—Å—Ç–∏–≤–∞–ª—å', '–∫–æ–Ω—Ü–µ—Ä—Ç', '–≤—ã—Å—Ç–∞–≤–∫–∞', '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ', '–ø—Ä–∞–∑–¥–Ω–∏–∫']

                municipal_count = sum(1 for word in municipal_keywords if word in text_lower)
                commercial_count = sum(1 for word in commercial_keywords if word in text_lower)
                event_count = sum(1 for word in event_keywords if word in text_lower)

                if municipal_count >= 2:
                    content_type = "–ú–£–ù–ò–¶–ò–ü–ê–õ–¨–ù–ê–Ø –ø—Ä–æ–±–ª–µ–º–∞"
                elif commercial_count >= 2:
                    content_type = "–ö–û–ú–ú–ï–†–ß–ï–°–ö–ê–Ø –Ω–æ–≤–æ—Å—Ç—å"
                elif event_count >= 1:
                    content_type = "–ú–ï–†–û–ü–†–ò–Ø–¢–ò–ï"
                else:
                    content_type = "–û–ë–©–ê–Ø –Ω–æ–≤–æ—Å—Ç—å"

                print(f"      üìä –¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {content_type}")
                # =======================================================

                # AI –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–∏
                try:
                    print(f"      ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é –Ω–∞ AI –∞–Ω–∞–ª–∏–∑...")

                    ai_results = analyze_news_article(
                        news_text[:1500],
                        source_url,
                        "parser",
                        parse_time if parse_time else datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    )

                    if ai_results and len(ai_results) > 0:
                        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –ò–ò
                        validated_data = process_ai_response(ai_results, source_url, parse_time)

                        if not validated_data:
                            print(f"      ‚è≠Ô∏è –û—Ç–≤–µ—Ç –ò–ò –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é")
                            continue

                        print(
                            f"      ‚úÖ AI –∞–Ω–∞–ª–∏–∑: {validated_data['category']} (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {validated_data['priority']})")
                        print(f"      üì§ –û—Ç–ø—Ä–∞–≤–ª—è—é –≤ –±—ç–∫–µ–Ω–¥...")

                        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –±—ç–∫–µ–Ω–¥
                        if send_to_backend(validated_data, source_url, parse_time):
                            processed_count += 1
                            print(f"      ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ! –í—Å–µ–≥–æ: {processed_count}")
                        else:
                            print(f"      ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ –±—ç–∫–µ–Ω–¥")

                    else:
                        print(f"      ‚ö†Ô∏è AI –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")

                except Exception as e:
                    print(f"      ‚ùå –û—à–∏–±–∫–∞ AI –∞–Ω–∞–ª–∏–∑–∞: {e}")

            else:
                print(f"      ‚ö†Ô∏è –¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π ({len(news_text)} —Å–∏–º–≤–æ–ª–æ–≤)")

        print(f"\nüéØ –ò–¢–û–ì–û: –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count} –Ω–æ–≤–æ—Å—Ç–µ–π")

        # –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã
        if processed_count > 0:
            clusters = cluster_similar_problems()
            print(f"üìä –°–æ–∑–¥–∞–Ω–æ {len(clusters)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø—Ä–æ–±–ª–µ–º")

        return processed_count

    except Exception as e:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ process_and_save_news: {e}")
        import traceback
        traceback.print_exc()
        return 0


# ========== –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ì–ï–ù–ï–†–ê–¶–ò–ò –û–¢–ß–ï–¢–ê ==========
def generate_report():
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ —Ç–µ–∫—É—â–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏"""
    try:
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É reports –µ—Å–ª–∏ –Ω–µ—Ç
        reports_dir = os.path.join(backend_dir, 'reports')
        os.makedirs(reports_dir, exist_ok=True)

        # –ò–º—è —Ñ–∞–π–ª–∞ –æ—Ç—á–µ—Ç–∞
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = os.path.join(reports_dir, f"report_{timestamp}.txt")

        db_path = os.path.join(backend_dir, 'data', 'municipal_monitoring.db')
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        cursor.execute('SELECT COUNT(*) FROM problems WHERE created_at > datetime("now", "-24 hours")')
        problems_24h = cursor.fetchone()[0]

        cursor.execute('SELECT COUNT(*) FROM problems WHERE priority >= 2')
        critical_problems = cursor.fetchone()[0]

        cursor.execute('SELECT COUNT(*) FROM clusters')
        clusters_count = cursor.fetchone()[0]

        cursor.execute('SELECT category, COUNT(*) as count FROM problems GROUP BY category ORDER BY count DESC LIMIT 5')
        top_categories = cursor.fetchall()

        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
        cursor.execute('''
            SELECT text, category, location, priority, created_at 
            FROM problems 
            ORDER BY created_at DESC 
            LIMIT 5
        ''')
        recent_problems = cursor.fetchall()

        conn.close()

        # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write(f"–û–¢–ß–ï–¢ AI-–ü–û–ú–û–©–ù–ò–ö–ê –ì–õ–ê–í–´ –ï–ö–ê–¢–ï–†–ò–ù–ë–£–†–ì–ê\n")
            f.write(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 60 + "\n\n")

            f.write("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n")
            f.write(f"  ‚Ä¢ –ü—Ä–æ–±–ª–µ–º –∑–∞ 24 —á–∞—Å–∞: {problems_24h}\n")
            f.write(f"  ‚Ä¢ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º: {critical_problems}\n")
            f.write(f"  ‚Ä¢ –ê–∫—Ç–∏–≤–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {clusters_count}\n\n")

            f.write("üèÜ –¢–û–ü-5 –ö–ê–¢–ï–ì–û–†–ò–ô –ü–†–û–ë–õ–ï–ú:\n")
            for category, count in top_categories:
                f.write(f"  ‚Ä¢ {category}: {count} –ø—Ä–æ–±–ª–µ–º\n")

            f.write("\nüö® –ü–û–°–õ–ï–î–ù–ò–ï –ü–†–û–ë–õ–ï–ú–´:\n")
            for problem in recent_problems:
                f.write(f"  ‚Ä¢ [{problem[3]}] {problem[1]}: {problem[0][:80]}...\n")
                f.write(f"    üìç {problem[2]} | üïê {problem[4][:19]}\n")

            f.write("\n" + "=" * 60 + "\n")
            f.write("üéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\n")
            f.write("1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å\n")
            f.write("2. –û–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n")
            f.write("3. –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –Ω–æ–≤—ã–µ –∏–Ω—Ü–∏–¥–µ–Ω—Ç—ã –∏–∑ –ø–∞—Ä—Å–µ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π\n")
            f.write("4. –ö–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É —Å–ª—É–∂–± –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º –ø—Ä–æ–±–ª–µ–º\n")
            f.write("=" * 60 + "\n")

        logger.info(f"‚úÖ –û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {report_file}")
        return report_file

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")
        return None


# ========== –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ==========
def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å–ª–æ—è"""
    logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å–ª–æ—è...")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ë–î
    init_database()

    logger.info("‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–ª–æ–π –≥–æ—Ç–æ–≤")

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å–ª–æ—è...")

    # –¢–µ—Å—Ç 1: –û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –ø—Ä–æ–±–ª–µ–º—ã
    test_problem = {
        "text": "–¢–µ—Å—Ç–æ–≤–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –æ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å–ª–æ—è",
        "category": "–¢–µ—Å—Ç",
        "location": "–¶–µ–Ω—Ç—Ä –≥–æ—Ä–æ–¥–∞",
        "sentiment": "neutral",
        "priority": 1,
        "metadata": json.dumps({
            "source_url": "test",
            "tags": ["—Ç–µ—Å—Ç"],
            "is_incident": False
        })
    }

    if send_to_backend(test_problem):
        print("‚úÖ –¢–µ—Å—Ç–æ–≤–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    else:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—É—é –ø—Ä–æ–±–ª–µ–º—É")

    # –¢–µ—Å—Ç 2: –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    clusters = create_clusters_from_problems()
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(clusters)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")

    # –¢–µ—Å—Ç 3: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—é
    location_clusters = cluster_similar_problems()
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(location_clusters)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—é")

    if location_clusters:
        print("\nüéØ –ü—Ä–∏–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
        for i, cluster in enumerate(location_clusters[:3]):
            print(f"   {i + 1}. {cluster['category']} –≤ {cluster['location']}")
            print(f"      –ß–∞—Å—Ç–æ—Ç–∞: {cluster['frequency']} —Ä–∞–∑")

    # –¢–µ—Å—Ç 4: –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
    news_count = process_and_save_news()
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {news_count} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –ø–∞—Ä—Å–µ—Ä–∞")

    # –¢–µ—Å—Ç 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ë–î
    try:
        db_path = os.path.join(backend_dir, 'data', 'municipal_monitoring.db')
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM problems")
        total_count = cursor.fetchone()[0]
        print(f"üìä –í –ë–î –≤—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total_count}")

        cursor.execute("SELECT text, category, location FROM problems ORDER BY created_at DESC LIMIT 3")
        recent = cursor.fetchall()
        print("üì∞ –ü–æ—Å–ª–µ–¥–Ω–∏–µ –∑–∞–ø–∏—Å–∏:")
        for problem in recent:
            print(f"   ‚Ä¢ {problem[1]}: {problem[0][:50]}...")

        conn.close()
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ë–î: {e}")

    # –¢–µ—Å—Ç 6: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    report_file = generate_report()
    if report_file:
        print(f"‚úÖ –û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {report_file}")
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—á–µ—Ç")

    print("\nüéØ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–ª–æ–π —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ...")
    print("üì° –û–∂–∏–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ—Ç –ø–∞—Ä—Å–µ—Ä–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏")

    # –ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
    while True:
        try:
            # –ö–∞–∂–¥—ã–π —á–∞—Å –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏ –æ–±–Ω–æ–≤–ª—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã
            time.sleep(3600)  # 1 —á–∞—Å

            logger.info("üîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π...")
            process_and_save_news()
            cluster_similar_problems()

        except KeyboardInterrupt:
            logger.info("üî¥ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–ª–æ–π –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            break
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
            time.sleep(300)  # –ñ–¥–µ–º 5 –º–∏–Ω—É—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ


if __name__ == "__main__":
    main()